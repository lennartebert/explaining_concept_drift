{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647f46ed-c149-4abc-b148-af8c0808c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "import os\n",
    "from concept_drift import generate_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb66805-c964-4415-af7f-b4b9f408f909",
   "metadata": {},
   "source": [
    "# Generate Synthetic Attribute Data with different settings\n",
    "Always generate 10 datasets with the same settings.\n",
    "\n",
    "Out of the 10 change points, the first 5 are always explainable with attributes 1, 2, 3, 4 and 5. The last 5 are not explainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc675551-b730-4da3-aa24-80f61db08636",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_event_logs = 1\n",
    "event_log_size = 2500\n",
    "relevant_attributes = 5\n",
    "irrelevant_attributes = 5\n",
    "attribute_value_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9774dbd-a0dd-46b4-a4f2-acb61b5f51af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data\\\\synthetic\\\\maardji et al 2013_xes\\\\logs\\\\cf\\\\cf2.5k.xes'],\n",
       "      dtype='<U58')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 random event logs from the maardji et al 2013 dataset that are of the desired size\n",
    "suitable_dataset= helper.get_datasets_by_criteria(size=event_log_size, \n",
    "                                             has_generated_attributes=False, \n",
    "                                             dataset='maardji et al 2013')\n",
    "datasets = np.random.choice(list(suitable_dataset.keys()), count_event_logs)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c04c89-9965-4049-8579-1cf35295c55c",
   "metadata": {},
   "source": [
    "## Scenario 1: New material, new vendor...\n",
    "\n",
    "Data is categorical. One new category is added which is sampled at 10%. All other categories are sampled at random distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854c284a-ffd5-4fb4-9668-8282ca63cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_distribution(attribute_value_count):\n",
    "    \"\"\"Get a single distribution for a given number of values.\n",
    "    \n",
    "    Args:\n",
    "        number_values: How many different values the distribution should have.\n",
    "    \n",
    "    Returns:\n",
    "        A probability distribution.\n",
    "    \"\"\"\n",
    "    # use the Dirichlet distribution\n",
    "    distribution = np.random.dirichlet(np.ones(attribute_value_count)).tolist()\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4ed31c-7e12-48a3-b846-3557da197fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value_probability = 0.2\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def _get_drifted_distributions(attribute_value_count, change_type=None):\n",
    "    \"\"\"Get two distributions, the baseline distribution and the drifted distribution.\n",
    "    \n",
    "    The change_type determines in which regard both are different.\n",
    "    \n",
    "    The two distributions are guaranteed to be significantly different at 10 observations.\n",
    "    \n",
    "    Args:\n",
    "        attribute_value_count: How many attribute values there are.\n",
    "        change_type: 'new_value', 'overproportional_gain' or 'independent_new'.\n",
    "    \n",
    "    Returns:\n",
    "        (baseline_distribution, drifted_distribution) tuple\n",
    "    \"\"\"\n",
    "    if attribute_value_count < 2: raise Exception('Must generate at least 2 attribute values.')\n",
    "    \n",
    "    if change_type is None:\n",
    "        change_type = np.random.choice(['new_value', 'overproportional_gain', 'independent_new'])\n",
    "    \n",
    "    # get the baseline distribution\n",
    "    baseline_distribution = None\n",
    "    if change_type == 'new_value':\n",
    "        baseline_distribution = _get_distribution(attribute_value_count - 1)\n",
    "        # add a 0% probability item\n",
    "        baseline_distribution.append(0)\n",
    "    else:\n",
    "        baseline_distribution = _get_distribution(attribute_value_count)\n",
    "        \n",
    "    drifted_distribution_found = False\n",
    "    drifted_distribution = None\n",
    "    \n",
    "    while not drifted_distribution_found:\n",
    "        if change_type == 'new_value':\n",
    "            drifted_distribution = baseline_distribution.copy()\n",
    "            drifted_distribution = list(np.array(drifted_distribution) * (1 - new_value_probability))\n",
    "            drifted_distribution[-1] = new_value_probability\n",
    "        else:\n",
    "            drifted_distribution = _get_distribution(attribute_value_count)\n",
    "        \n",
    "        hellinger_distance = np.sqrt(np.sum((np.sqrt(baseline_distribution) - np.sqrt(drifted_distribution)) ** 2)) / np.sqrt(2)\n",
    "        \n",
    "        if hellinger_distance > 0.3:\n",
    "            drifted_distribution_found = True\n",
    "\n",
    "    return baseline_distribution, drifted_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075c5beb-3311-41f0-ab9f-d837e87b4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset 0\n",
      "Importance: DEBUG\n",
      "Message: Start serializing log to XES.XML\n",
      "\n",
      "Importance: DEBUG\n",
      "Message: finished serializing log (7858.00048828125 msec.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_path = 'data/synthetic/generated/new_attribute_values/'\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f'Generating dataset {i}')\n",
    "    dataset_info = helper.get_data_information(dataset)\n",
    "    opyenxes_log = helper.opyenxes_read_xes(dataset_info['file_path'])\n",
    "    ag = generate_attributes.AttributeGenerator(opyenxes_log, dataset_info['change_points'])\n",
    "    \n",
    "    # generate drifted attributes\n",
    "    for attribute_index in range(relevant_attributes):\n",
    "        attribute_name = f'relevant_attribute_{attribute_index + 1}'\n",
    "        \n",
    "        # get the distributions\n",
    "        base_distribution, drifted_distribution = _get_drifted_distributions(attribute_value_count)\n",
    "\n",
    "        # change point to explain\n",
    "        explain_change_point = dataset_info['change_points'][attribute_index]\n",
    "        ag.generate_drifting_categorical_attribute(attribute_name,\n",
    "                                          base_distribution,\n",
    "                                          drifted_distribution=drifted_distribution,\n",
    "                                          explain_change_point=explain_change_point,\n",
    "                                          drift_type = 'sudden')\n",
    "    \n",
    "    # generate attributes that did not drift\n",
    "    for attribute_index in range(irrelevant_attributes):\n",
    "        attribute_name = f'irrelevant_attribute_{attribute_index + 1}'\n",
    "        distribution = _get_distribution(attribute_value_count)\n",
    "        ag.generate_categorical_attribute(attribute_name, distribution)\n",
    "    \n",
    "    # prepare to write results\n",
    "    file_name = f'{event_log_size}_sudden_{relevant_attributes}_{i:02d}.xes'\n",
    "    out_file_path = os.path.join(out_path, file_name)\n",
    "    out_file_path = os.path.normpath(out_file_path)\n",
    "    \n",
    "    # write event log to file\n",
    "    helper.opyenxes_write_xes(ag.opyenxes_log, out_file_path)\n",
    "    \n",
    "    # save the data information\n",
    "    dataset_info['file_path'] = out_file_path\n",
    "    dataset_info['file_name'] = file_name\n",
    "    dataset_info['has_generated_attributes'] = True\n",
    "    dataset_info['change_point_explanations'] = ag.change_point_explanations\n",
    "    helper.update_data_dictionary({out_file_path: dataset_info})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
